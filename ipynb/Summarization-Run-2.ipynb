{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gen_sent_vectors(clean_sentences):\n",
    "    \"\"\"\n",
    "    Generate sentence vectors using word embeddings.\n",
    "    \"\"\"\n",
    "    sentence_vectors = []\n",
    "    for i in clean_sentences:\n",
    "        if len(i) != 0:\n",
    "            v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "        else:\n",
    "            v = np.zeros((100,))\n",
    "        sentence_vectors.append(v)\n",
    "    return sentence_vectors\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def compute_similarity_matrix(one_complaint, sentence_vectors):\n",
    "    \"\"\"Compute similarity matrix between sentences within one complaint\"\"\"\n",
    "    sim_mat = np.zeros([len(one_complaint), len(one_complaint)])\n",
    "    for i in range(len(one_complaint)):\n",
    "        for j in range(len(one_complaint)):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1, 100),\n",
    "                                                 sentence_vectors[j].reshape(1, 100))[0, 0]\n",
    "    return sim_mat\n",
    "\n",
    "import networkx as nx\n",
    "def rank_sentences(sim_mat, one_complaint, maxlen):\n",
    "    nx_graph = nx.from_numpy_array(sim_mat)\n",
    "    try: # Note that PageRank is not guaranteed to converge, hence return the same sentences\n",
    "        scores = nx.pagerank(nx_graph, max_iter=100)\n",
    "    except Exception:\n",
    "        scores = np.zeros(len(one_complaint))\n",
    "    \n",
    "    ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(one_complaint)), reverse=True)\n",
    "    return ranked_sentences\n",
    "\n",
    "def gen_top_n_sentences(one_complaint, clean_sentences, maxlen=5):\n",
    "    \"\"\"Get the top n most relevant sentences of each of the complaints.\n",
    "    This is done by using the TextRank algorithm, in the same spirit\n",
    "    as the PageRank algorithm, except the nodes of the graph are \n",
    "    sentences instead of webpages.\n",
    "    \"\"\"\n",
    "    if len(one_complaint) < maxlen:\n",
    "        condensed = ' '.join(one_complaint)\n",
    "    else:\n",
    "        sentence_vectors = gen_sent_vectors(clean_sentences)\n",
    "   \n",
    "        sim_mat = compute_similarity_matrix(one_complaint, sentence_vectors)\n",
    "        ranked_sentences = rank_sentences(sim_mat, one_complaint, maxlen)\n",
    "        condensed_sent = [sent[1] for i, sent in enumerate(ranked_sentences) if i < maxlen]\n",
    "        \n",
    "        condensed = ' '.join(re.sub('xx\\\\S+|XX\\\\S+', ' ', ' '.join(condensed_sent)).split())\n",
    "    return condensed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
